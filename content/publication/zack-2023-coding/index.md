---
title: "Coding Inequity: Assessing GPT-4's Potential for Perpetuating Racial and Gender Biases in Healthcare"
date: 2023-01-01
publishDate: 2023-09-07T16:53:34.391411Z
authors: ["Travis Zack", "Eric Lehman", "Mirac Suzgun", "Jorge A Rodriguez", "Leo Anthony Celi", "Judy Gichoya", "Dan Jurafsky", "Peter Szolovits", "David W Bates", "Raja-Elie E Abdulnour", "Atul Butte", "Emily Alsentzer"]
publication_types: ["2"]
abstract: "Background Large language models (LLMs) such as GPT-4 hold great promise as transformative tools in healthcare, ranging from automating administrative tasks to augmenting clinical decision- making. However, these models also pose a serious danger of perpetuating biases and delivering incorrect medical diagnoses, which can have a direct, harmful impact on medical care.

Methods Using the Azure OpenAI API, we tested whether GPT-4 encodes racial and gender biases and examined the impact of such biases on four potential applications of LLMs in the clinical domainâ€”namely, medical education, diagnostic reasoning, plan generation, and patient assessment. We conducted experiments with prompts designed to resemble typical use of GPT-4 within clinical and medical education applications. We used clinical vignettes from NEJM Healer and from published research on implicit bias in healthcare. GPT-4 estimates of the demographic distribution of medical conditions were compared to true U.S. prevalence estimates. Differential diagnosis and treatment planning were evaluated across demographic groups using standard statistical tests for significance between groups.

Findings We find that GPT-4 does not appropriately model the demographic diversity of medical conditions, consistently producing clinical vignettes that stereotype demographic presentations. The differential diagnoses created by GPT-4 for standardized clinical vignettes were more likely to include diagnoses that stereotype certain races, ethnicities, and gender identities. Assessment and plans created by the model showed significant association between demographic attributes and recommendations for more expensive procedures as well as differences in patient perception.

Interpretation Our findings highlight the urgent need for comprehensive and transparent bias assessments of LLM tools like GPT-4 for every intended use case before they are integrated into clinical care. We discuss the potential sources of these biases and potential mitigation strategies prior to clinical implementation."
featured: false
publication: "*medRxiv*"
url_pdf: "https://www.medrxiv.org/content/10.1101/2023.07.13.23292577v1#:~:text=Findings%20We%20find%20that%20GPT,vignettes%20that%20stereotype%20demographic%20presentations."
links: 
- name: News Coverage
  url: https://www.statnews.com/2023/07/18/gpt4-health-disease-diagnosis-treatment-ai/
---

